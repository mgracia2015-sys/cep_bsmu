import streamlit as st
import re
from docx import Document
from docx.shared import Pt, Cm
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from io import BytesIO

# --- –§–£–ù–ö–¶–Ü–á –§–û–†–ú–ê–¢–£–í–ê–ù–ù–Ø (–ë–ï–ó –ó–ú–Ü–ù) ---

def apply_base_style(paragraph, first_line=1.25, space_before=0):
    paragraph.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
    paragraph.paragraph_format.line_spacing = 1.15
    paragraph.paragraph_format.first_line_indent = Cm(first_line)
    paragraph.paragraph_format.space_before = Pt(space_before)

def add_run(paragraph, text, bold=False, italic=False):
    run = paragraph.add_run(text)
    run.font.name = 'Times New Roman'
    run.font.size = Pt(12)
    run.bold, run.italic = bold, italic
    run._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')
    return run

def fix_authors_metadata(text):
    parts = re.split(r'[,;]', text)
    fixed = []
    for p in parts:
        p = p.strip()
        res = re.sub(r'([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]+)\s+([–ê-–Ø–Å–Ü–á–Ñ“êA-Z]\.\s?[–ê-–Ø–Å–Ü–á–Ñ“êA-Z]\.)', r'\2 \1', p)
        fixed.append(res)
    return ", ".join(fixed)

def format_vancouver(text):
    text = text.replace('"', '').replace('¬´', '').replace('¬ª', '')
    text = re.sub(r'([A-Z–ê-–Ø][a-z–∞-—è]+)\s+([A-Z–ê-–Ø])\.\s?([A-Z–ê-–Ø])\.', r'\1 \2\3', text)
    text = re.sub(r'([A-Z–ê-–Ø][a-z–∞-—è]+)\s+([A-Z–ê-–Ø])\.', r'\1 \2', text)
    text = re.sub(r'(\d{4})[\.\s,‚Äì‚Äî]*Vol\.?\s*(\d+)[\.\s,‚Äì‚Äî]*[Nn]o\.?\s*(\d+)[\.\s,‚Äì‚Äî]*[Pp]\.?\s*(\d+)[-‚Äì‚Äî](\d+)', r'\1;\2(\3):\4-\5', text)
    text = re.sub(r'(\d{4})[\.\s,‚Äì‚Äî]*Vol\.?\s*(\d+)[\.\s,‚Äì‚Äî]*[Pp]\.?\s*(\d+)[-‚Äì‚Äî](\d+)', r'\1;\2:\3-\4', text)
    return text.strip()

def process_abstract_block(new_doc, raw_text, terms, forbidden_word, lang_label, report, skip_warnings=False):
    clean_text = re.sub(rf'^{forbidden_word}[:\s.-]*', '', raw_text, flags=re.IGNORECASE).strip()
    if lang_label == "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞" and len(clean_text) > 1600:
        report.append(f"‚ö†Ô∏è {lang_label} –∞–Ω–æ—Ç–∞—Ü—ñ—è –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∞ ({len(clean_text)} –∑–Ω. –ø—Ä–∏ –ª—ñ–º—ñ—Ç—ñ 1600).")
    
    if not skip_warnings:
        for t in terms:
            if t not in clean_text: 
                report.append(f"‚ùå –í–Ü–î–°–£–¢–ù–Ü–ô —Ä–æ–∑–¥—ñ–ª —É {lang_label} –∞–Ω–æ—Ç–∞—Ü—ñ—ó: {t}")

    pattern = f"({'|'.join(re.escape(t) for t in terms)})"
    parts = re.split(pattern, clean_text)
    curr_term = None
    for pt in parts:
        if not pt or not pt.strip(): continue
        if pt in terms: curr_term = pt
        else:
            p = new_doc.add_paragraph()
            if curr_term: add_run(p, curr_term, bold=True, italic=True)
            add_run(p, " " + pt.strip())
            apply_base_style(p); curr_term = None

# --- –Ü–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

st.set_page_config(page_title="–ù–∞—É–∫–æ–≤–∏–π –†–µ–¥–∞–∫—Ç–æ—Ä", page_icon="üìù")
st.title("üìù –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∞—Ç—Ç—ñ")

# –í–∏–±—ñ—Ä —Ç–∏–ø—É —Å—Ç–∞—Ç—Ç—ñ (–î–æ–¥–∞–Ω–æ —Ç—Ä–µ—Ç—ñ–π –≤–∞—Ä—ñ–∞–Ω—Ç)
article_type = st.radio(
    "–û–±–µ—Ä—ñ—Ç—å —Ç–∏–ø –≤–∞—à–æ—ó —Å—Ç–∞—Ç—Ç—ñ:",
    ("–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è", "–ö–ª—ñ–Ω—ñ—á–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫", "–û–≥–ª—è–¥ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏")
)

is_clinical = (article_type == "–ö–ª—ñ–Ω—ñ—á–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫")
is_review = (article_type == "–û–≥–ª—è–¥ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏")

uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª .docx", type="docx")

if uploaded_file is not None:
    if st.button("–û–±—Ä–æ–±–∏—Ç–∏ —Å—Ç–∞—Ç—Ç—é"):
        try:
            doc = Document(uploaded_file)
            new_doc = Document()
            report = []
            paras = [p for p in doc.paragraphs if p.text.strip()]
            
            # 1. –®–ê–ü–ö–ê
            p_udc = new_doc.add_paragraph(); add_run(p_udc, paras[0].text); apply_base_style(p_udc)
            p_t_ua = new_doc.add_paragraph(); add_run(p_t_ua, paras[1].text.upper()); apply_base_style(p_t_ua)
            p_a_ua = new_doc.add_paragraph(); add_run(p_a_ua, fix_authors_metadata(paras[2].text), bold=True, italic=True); apply_base_style(p_a_ua)
            p_aff = new_doc.add_paragraph(); add_run(p_aff, paras[3].text); apply_base_style(p_aff)

            ua_kw_idx = next((i for i, p in enumerate(paras) if "–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞" in p.text), -1)
            en_kw_idx = next((i for i, p in enumerate(paras) if "Key words" in p.text or "Keywords" in p.text), -1)

            # 2. –ê–ù–û–¢–ê–¶–Ü–á (–õ–æ–≥—ñ–∫–∞ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ç–∏–ø—É)
            if is_review:
                ua_terms = ["–ú–µ—Ç–∞", "–í–∏—Å–Ω–æ–≤–∫–∏"]
                en_terms = ["Aim", "Conclusions"]
            elif is_clinical:
                ua_terms = ["–ú–µ—Ç–∞", "–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ —ñ –º–µ—Ç–æ–¥–∏", "–†–µ–∑—É–ª—å—Ç–∞—Ç–∏", "–í–∏—Å–Ω–æ–≤–∫–∏"]
                en_terms = ["Aim", "Material and methods", "Results", "Conclusions"]
            else: # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è
                ua_terms = ["–ú–µ—Ç–∞", "–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ —ñ –º–µ—Ç–æ–¥–∏", "–†–µ–∑—É–ª—å—Ç–∞—Ç–∏", "–í–∏—Å–Ω–æ–≤–∫–∏"]
                en_terms = ["Aim", "Material and methods", "Results", "Conclusions"]

            process_abstract_block(new_doc, " ".join([paras[i].text for i in range(4, ua_kw_idx)]), 
                                   ua_terms, "–ê–Ω–æ—Ç–∞—Ü—ñ—è|–†–µ—Ñ–µ—Ä–∞—Ç", "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞", report, skip_warnings=is_clinical)
            
            p_kw_ua = new_doc.add_paragraph(); add_run(p_kw_ua, "–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞:", bold=True, italic=True)
            add_run(p_kw_ua, " " + paras[ua_kw_idx].text.replace("–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞", "").replace(":", "").strip()); apply_base_style(p_kw_ua)

            p_t_en = new_doc.add_paragraph(); add_run(p_t_en, paras[ua_kw_idx + 1].text.upper()); apply_base_style(p_t_en)
            p_a_en = new_doc.add_paragraph(); add_run(p_a_en, fix_authors_metadata(paras[ua_kw_idx + 2].text), bold=True, italic=True); apply_base_style(p_a_en)

            process_abstract_block(new_doc, " ".join([paras[i].text for i in range(ua_kw_idx + 3, en_kw_idx)]), 
                                   en_terms, "Abstract", "–ê–Ω–≥–ª—ñ–π—Å—å–∫–∞", report, skip_warnings=is_clinical)
            
            p_kw_en = new_doc.add_paragraph(); add_run(p_kw_en, "Key words:", bold=True, italic=True)
            add_run(p_kw_en, " " + paras[en_kw_idx].text.replace("Key words", "").replace("Keywords", "").replace(":", "").strip()); apply_base_style(p_kw_en)

            # 3. –û–°–ù–û–í–ù–ò–ô –¢–ï–ö–°–¢ (–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª—ñ–≤)
            if is_review:
                sections_map = [
                    (r"^–í—Å—Ç—É–ø", "–í—Å—Ç—É–ø"), 
                    (r"^–ú–µ—Ç–∞", "–ú–µ—Ç–∞ —Ä–æ–±–æ—Ç–∏"), 
                    (r"^–û—Å–Ω–æ–≤–Ω–∞\s+—á–∞—Å—Ç–∏–Ω–∞", "–û—Å–Ω–æ–≤–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞"), 
                    (r"^–í–∏—Å–Ω–æ–≤–æ–∫|^–í–∏—Å–Ω–æ–≤–∫–∏", "–í–∏—Å–Ω–æ–≤–∫–∏"),
                    (r"^–°–ø–∏—Å–æ–∫\s*–ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏|^–õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∞|^–°–ø–∏—Å–æ–∫\s*–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö\s*–¥–∂–µ—Ä–µ–ª", "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏")
                ]
                all_req = ["–í—Å—Ç—É–ø", "–ú–µ—Ç–∞ —Ä–æ–±–æ—Ç–∏", "–û—Å–Ω–æ–≤–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞", "–í–∏—Å–Ω–æ–≤–∫–∏", "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏"]
            elif is_clinical:
                sections_map = [(r"^–í—Å—Ç—É–ø", "–í—Å—Ç—É–ø"), (r"^–û–ø–∏—Å\s+–∫–ª—ñ–Ω—ñ—á–Ω–æ–≥–æ\s+–≤–∏–ø–∞–¥–∫—É", "–û–ø–∏—Å –∫–ª—ñ–Ω—ñ—á–Ω–æ–≥–æ –≤–∏–ø–∞–¥–∫—É"), (r"^–í–∏—Å–Ω–æ–≤–æ–∫|^–í–∏—Å–Ω–æ–≤–∫–∏", "–í–∏—Å–Ω–æ–≤–æ–∫"), (r"^–°–ø–∏—Å–æ–∫\s*–ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏|^–õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∞|^–°–ø–∏—Å–æ–∫\s*–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö\s*–¥–∂–µ—Ä–µ–ª", "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏")]
                all_req = ["–í—Å—Ç—É–ø", "–û–ø–∏—Å –∫–ª—ñ–Ω—ñ—á–Ω–æ–≥–æ –≤–∏–ø–∞–¥–∫—É", "–í–∏—Å–Ω–æ–≤–æ–∫", "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏"]
            else: # –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è
                sections_map = [(r"^–í—Å—Ç—É–ø", "–í—Å—Ç—É–ø"), (r"^–ú–µ—Ç–∞", "–ú–µ—Ç–∞ —Ä–æ–±–æ—Ç–∏"), (r"^–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏\s*(—ñ|—Ç–∞)\s*–º–µ—Ç–æ–¥–∏", "–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ —Ç–∞ –º–µ—Ç–æ–¥–∏ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è"), (r"^–†–µ–∑—É–ª—å—Ç–∞—Ç–∏\s*—Ç–∞\s*—ó—Ö\s*–æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è", "–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–∞ —ó—Ö –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è"), (r"^–í–∏—Å–Ω–æ–≤–∫–∏", "–í–∏—Å–Ω–æ–≤–∫–∏"), (r"^–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏\s*–ø–æ–¥–∞–ª—å—à–∏—Ö\s*–¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å", "–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏ –ø–æ–¥–∞–ª—å—à–∏—Ö –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å"), (r"^–°–ø–∏—Å–æ–∫\s*–ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏|^–õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∞|^–°–ø–∏—Å–æ–∫\s*–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö\s*–¥–∂–µ—Ä–µ–ª", "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏")]
                all_req = ["–í—Å—Ç—É–ø", "–ú–µ—Ç–∞ —Ä–æ–±–æ—Ç–∏", "–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ —Ç–∞ –º–µ—Ç–æ–¥–∏ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è", "–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–∞ —ó—Ö –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è", "–í–∏—Å–Ω–æ–≤–∫–∏", "–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏ –ø–æ–¥–∞–ª—å—à–∏—Ö –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å", "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏"]
            
            in_literature = False
            in_references = False
            found_sections = set()

            for i in range(en_kw_idx + 1, len(paras)):
                text = paras[i].text.strip()
                if re.match(r"^References[:.\s]*$", text, re.IGNORECASE):
                    p_ref = new_doc.add_paragraph(); add_run(p_ref, "References", bold=True); apply_base_style(p_ref); in_references = True; in_literature = False; continue
                matched_std = None
                for pattern, std_name in sections_map:
                    if re.match(pattern, text, re.IGNORECASE):
                        matched_std = std_name; text = re.sub(pattern + r"[:.\s-]*", "", text, count=1, flags=re.IGNORECASE).strip(); break
                
                if matched_std:
                    p_h = new_doc.add_paragraph(); add_run(p_h, matched_std, bold=True); apply_base_style(p_h, space_before=10); found_sections.add(matched_std); in_literature = (matched_std == "–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏")
                    if text: p_c = new_doc.add_paragraph(); add_run(p_c, format_vancouver(text) if in_literature else text); apply_base_style(p_c)
                else:
                    p_txt = new_doc.add_paragraph()
                    if in_literature or in_references:
                        vanc_text = format_vancouver(text); add_run(p_txt, vanc_text)
                    else: add_run(p_txt, text)
                    apply_base_style(p_txt)

            for r in all_req:
                if r not in found_sections: report.append(f"‚ùå –ù–ï –ó–ù–ê–ô–î–ï–ù–û –†–û–ó–î–Ü–õ: {r}")

            bio = BytesIO()
            new_doc.save(bio)
            
            st.subheader("–ó–≤—ñ—Ç –ø—Ä–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É:")
            if not report: st.success("‚úÖ –í—Å–µ –≤–∏–≥–ª—è–¥–∞—î —á—É–¥–æ–≤–æ!")
            else:
                for issue in report:
                    if "‚ùå" in issue: st.error(issue)
                    else: st.warning(issue)

            st.download_button(label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω—É —Å—Ç–∞—Ç—Ç—é", data=bio.getvalue(), file_name=f"fixed_{uploaded_file.name}", mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
        except Exception as e: st.error(f"–°—Ç–∞–ª–∞—Å—è –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ: {e}")